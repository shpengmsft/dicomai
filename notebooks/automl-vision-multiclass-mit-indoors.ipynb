{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/ignore-notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621827794293
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Run, Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "import azureml.dataprep as dprep\n",
    "from azureml.core.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621827796397
    }
   },
   "outputs": [],
   "source": [
    "# Load workspace\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621827796691
    }
   },
   "outputs": [],
   "source": [
    "# Choose a name for the run history container in the workspace.\n",
    "experiment_name = 'mit-indoors-multiclass'\n",
    "project_folder = './project'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace Name'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for your AutoML run. In this tutorial, you create `AmlCompute` as your training compute resource.\n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace this code will skip the creation process.\n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621827797538
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"gpu-cluster\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "\n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_NC6\",\n",
    "                                                                max_nodes = 4)\n",
    "    # Create the cluster.\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "\n",
    "# For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1621833561191
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "if 'datasets' in ws.datastores.keys():\n",
    "    ds = ws.datastores['datasets']\n",
    "else:\n",
    "    account_key = os.getenv(\"ACCOUNT_KEY\")\n",
    "    ds = Datastore.register_azure_blob_container(ws, datastore_name='datasets', container_name='imagecontainer', \n",
    "                                             account_name='dicommodel2290602728', account_key=account_key, \n",
    "                                             resource_group='dicom-model-rg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621827801307
    }
   },
   "outputs": [],
   "source": [
    "from azureml.contrib.dataset.labeled_dataset import _LabeledDatasetFactory, LabeledDatasetTask\n",
    "from azureml.core import Dataset\n",
    "\n",
    "# create training dataset\n",
    "training_dataset_name = experiment_name + \"_multiclass_training_dataset\"\n",
    "if training_dataset_name in ws.datasets:\n",
    "    training_dataset = ws.datasets.get(training_dataset_name)\n",
    "    print('Found the dataset', training_dataset_name)\n",
    "else:\n",
    "    training_dataset = _LabeledDatasetFactory.from_json_lines(\n",
    "        task=LabeledDatasetTask.IMAGE_CLASSIFICATION, path=ds.path('mit_indoors/TrainImages_with_label.jsonl'))\n",
    "    training_dataset = training_dataset.register(workspace=ws, name=training_dataset_name)\n",
    "\n",
    "# create validation dataset\n",
    "validation_dataset_name = experiment_name + \"_multiclass_validation_dataset\"\n",
    "if validation_dataset_name in ws.datasets:\n",
    "    validation_dataset = ws.datasets.get(validation_dataset_name)\n",
    "    print('Found the dataset', validation_dataset_name)\n",
    "else:\n",
    "    validation_dataset = _LabeledDatasetFactory.from_json_lines(\n",
    "        task=LabeledDatasetTask.IMAGE_CLASSIFICATION, path=ds.path('mit_indoors/TestImages_with_label.jsonl'))\n",
    "    validation_dataset = validation_dataset.register(workspace=ws, name=validation_dataset_name)\n",
    "\n",
    "print(\"Training dataset name: \" + training_dataset.name)\n",
    "print(\"Validation dataset name: \" + validation_dataset.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621827801482
    }
   },
   "outputs": [],
   "source": [
    "project_folder = './project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621827801630
    }
   },
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"deterministic\": True,\n",
    "    \"enable_dnn\": True,\n",
    "    \"featurization\": \"off\",\n",
    "    \"iteration_timeout_minutes\": 120,\n",
    "    \"iterations\": 1,\n",
    "    \"log_verbose_metrics\": True,\n",
    "    \"primary_metric\": \"accuracy\",\n",
    "    \"print_local_package_versions\": True,\n",
    "    \"seed\" : 47,\n",
    "    \"verbosity\": logging.INFO\n",
    "}\n",
    "\n",
    "if os.getenv(\"SCENARIO\"):\n",
    "    automl_settings[\"scenario\"] = os.getenv(\"SCENARIO\")\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'image-classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             path = project_folder,\n",
    "                             compute_target=compute_target,\n",
    "                             training_data=training_dataset,\n",
    "                             validation_data=validation_dataset,\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621827808742
    }
   },
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621827808899
    }
   },
   "outputs": [],
   "source": [
    "remote_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621831611271
    }
   },
   "outputs": [],
   "source": [
    "remote_run.wait_for_completion(wait_post_processing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621833137734
    }
   },
   "outputs": [],
   "source": [
    "# Load training script run corresponding to AutoML run above.\n",
    "training_run_id = remote_run.id + \"_HD_0\"\n",
    "training_run = Run(experiment, training_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621833566851
    }
   },
   "outputs": [],
   "source": [
    "# Count number of lines in ds for comparison\n",
    "image_class_list_file_path = 'mit_indoors/TestImages_with_label.jsonl'\n",
    "ds.download('.', prefix=image_class_list_file_path, overwrite=True)\n",
    "with open(image_class_list_file_path) as fp:\n",
    "    number_of_lines_in_validation_dataset = len(fp.readlines())\n",
    "    print(\"Number of lines in validation dataset: {}\".format(number_of_lines_in_validation_dataset))\n",
    "\n",
    "# Use validation dataset as inference dataset\n",
    "print(\"Inference dataset name: \" + validation_dataset.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621833573352
    }
   },
   "outputs": [],
   "source": [
    "# Inference script run arguments\n",
    "arguments = [\n",
    "        \"--run_id\", training_run_id,\n",
    "        \"--experiment_name\", experiment.name,\n",
    "        \"--input_dataset_id\", validation_dataset.id\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621833578882
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from azureml.core.script_run_config import ScriptRunConfig\n",
    "\n",
    "output_feature_file = \"./outputs/features.txt\"\n",
    "featurization_args = arguments + [\"--output_file\", output_feature_file]\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    # Download required files from training run into temp folder.\n",
    "    entry_script_name = \"featurize_script.py\"\n",
    "    output_path = os.path.join(tmpdir, entry_script_name)\n",
    "    training_run.download_file(\"train_artifacts/\" + entry_script_name, os.path.join(tmpdir, entry_script_name))\n",
    "    \n",
    "    script_run_config = ScriptRunConfig(source_directory=tmpdir,\n",
    "                                        script=entry_script_name,\n",
    "                                        compute_target=compute_target,\n",
    "                                        environment=training_run.get_environment(),\n",
    "                                        arguments=featurization_args)\n",
    "    featurization_run = experiment.submit(script_run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621833593560
    }
   },
   "outputs": [],
   "source": [
    "featurization_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621834041424
    }
   },
   "outputs": [],
   "source": [
    "featurization_run.wait_for_completion(wait_post_processing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621834046708
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "featurization_run.download_file(output_feature_file, output_file_path=output_feature_file)\n",
    "with open(output_feature_file) as features:\n",
    "    lines = features.readlines()\n",
    "    number_of_lines_in_feature_file = len(lines)\n",
    "    for line in lines:\n",
    "        obj = json.loads(line.strip())\n",
    "        assert 'filename' in obj\n",
    "        assert 'feature_vector' in obj\n",
    "        assert len(obj['feature_vector']) > 0\n",
    "    print(\"Number of lines in feature file: {}\".format(number_of_lines_in_feature_file))\n",
    "assert number_of_lines_in_feature_file == number_of_lines_in_validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621834049194
    }
   },
   "outputs": [],
   "source": [
    "output_prediction_file = \"./outputs/predictions.txt\"\n",
    "scoring_args = arguments + [\"--output_file\", output_prediction_file]\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    # Download required files from training run into temp folder.\n",
    "    entry_script_name = \"score_script.py\"\n",
    "    output_path = os.path.join(tmpdir, entry_script_name)\n",
    "    training_run.download_file(\"train_artifacts/\" + entry_script_name, os.path.join(tmpdir, entry_script_name))\n",
    "    \n",
    "    script_run_config = ScriptRunConfig(source_directory=tmpdir,\n",
    "                                        script=entry_script_name,\n",
    "                                        compute_target=compute_target,\n",
    "                                        environment=training_run.get_environment(),\n",
    "                                        arguments=scoring_args)\n",
    "    scoring_run = experiment.submit(script_run_config)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621834049620
    }
   },
   "outputs": [],
   "source": [
    "scoring_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621834156708
    }
   },
   "outputs": [],
   "source": [
    "scoring_run.wait_for_completion(wait_post_processing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1621834157470
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "scoring_run.download_file(output_prediction_file, output_file_path=output_prediction_file)\n",
    "with open(output_prediction_file) as predictions:\n",
    "    number_of_lines_in_prediction_file = len(predictions.readlines())\n",
    "    print(\"Number of lines in prediction file: {}\".format(number_of_lines_in_prediction_file))\n",
    "assert number_of_lines_in_prediction_file == number_of_lines_in_validation_dataset"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "hichando"
   }
  ],
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "name": "python3810jvsc74a57bd0eadf4def0511d16cff3b403ebad030b79626359e624f736446cdd8402c7991c1",
   "display_name": "Python 3.8.10 64-bit ('fhl_py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}