{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd0eadf4def0511d16cff3b403ebad030b79626359e624f736446cdd8402c7991c1",
   "display_name": "Python 3.8.10 64-bit ('fhl_py38': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run,Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load workspace\n",
    "ws = Workspace.from_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tests  = []\n",
    "# NoWindow\n",
    "test_no_window = {}\n",
    "test_no_window['ExperimentName'] = 'NoWindow'\n",
    "test_no_window['Comment'] = 'Imbalanced dataset, PNG image generated without futher window and level.'\n",
    "test_no_window['SoreRunId'] = 'NoWindow_1622062464_86b235aa'\n",
    "test_no_window['TestDataset'] = 'NoWindow_test'\n",
    "test_no_window['TestDatasetId'] = '2a63d9d0-dd6b-479d-bc01-da7a7b065ac4'\n",
    "test_no_window['y_pred'] = []\n",
    "test_no_window['y_true'] = []\n",
    "classification_tests.append(test_no_window)\n",
    "\n",
    "# balanced\n",
    "test_balanced = {}\n",
    "test_balanced['ExperimentName'] = 'Balanced_NoWindow'\n",
    "test_balanced['Comment'] = 'Balanced dataset, PNG image generated without futher window and level.'\n",
    "test_balanced['SoreRunId'] = 'Balanced_NoWindow_1622103656_29450fdf'\n",
    "test_balanced['TestDataset'] = 'Balanced_NoWindow_test'\n",
    "test_balanced['TestDatasetId'] = '2d8d0910-90ce-4c02-81d4-55abb2f8778c'\n",
    "test_balanced['y_pred'] = []\n",
    "test_balanced['y_true'] = []\n",
    "classification_tests.append(test_balanced)\n",
    "\n",
    "# 100by100\n",
    "test_100by100 = {}\n",
    "test_100by100['ExperimentName'] = '100by100'\n",
    "test_100by100['Comment'] = 'Imbalanced dataset, PNG image generated with window center=100, window width = 100.'\n",
    "test_100by100['SoreRunId'] = '100by100_1622086790_52811ae4'\n",
    "test_100by100['TestDataset'] = '100by100_test'\n",
    "test_100by100['TestDatasetId'] = 'bc69d1e3-2f54-420d-856d-69de440e3815'\n",
    "test_100by100['y_pred'] = []\n",
    "test_100by100['y_true'] = []\n",
    "classification_tests.append(test_100by100)\n",
    "\n",
    "# watermark\n",
    "test_watermark = {}\n",
    "test_watermark['ExperimentName'] = 'WaterMarked'\n",
    "test_watermark['Comment'] = 'Balanced dataset, PNG image generated without futher window and level, with WaterMark of Gender and ViewPosition.'\n",
    "test_watermark['SoreRunId'] = 'WaterMarked_1622526402_155992c8'\n",
    "test_watermark['TestDataset'] = 'WaterMarked_test'\n",
    "test_watermark['TestDatasetId'] = '44289778-b8e6-40c0-9334-f950e6c874e9'\n",
    "test_watermark['y_pred'] = []\n",
    "test_watermark['y_true'] = []\n",
    "classification_tests.append(test_watermark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def get_predictions(experiment_name: str, run_id: str) -> dict:\n",
    "    experiment = Experiment(ws, experiment_name)\n",
    "    scoring_run = Run(experiment, run_id)\n",
    "    output_prediction_file = \"./outputs/predictions.txt\"\n",
    "\n",
    "    scoring_run.download_file(output_prediction_file, output_file_path=output_prediction_file)\n",
    "    predicted_labels_with_filename = {}\n",
    "    with open(output_prediction_file) as predictions:\n",
    "        predict_str = predictions.readline()\n",
    "        target_class_mit_indoor = json.loads(predict_str)[\"labels\"]\n",
    "        while predict_str: \n",
    "            prediction_json_line = json.loads(predict_str)\n",
    "            labels = prediction_json_line[\"labels\"]\n",
    "            probs = prediction_json_line[\"probs\"]\n",
    "            \n",
    "            max_prob = max(probs)\n",
    "            max_index = probs.index(max_prob)\n",
    "            label_with_max_prob = labels[max_index]\n",
    "            \n",
    "            predicted_labels_with_filename[prediction_json_line[\"filename\"]] = label_with_max_prob\n",
    "            predict_str = predictions.readline()\n",
    "    return predicted_labels_with_filename\n",
    "\n",
    "# ground truth\n",
    "def get_ground_truth(experiment_name: str) -> dict:\n",
    "    default_ds = ws.datastores['workspaceblobstore']\n",
    "    image_class_list_file_path = 'fhl/datasets/'+ test['ExperimentName'] + '/label/labeleddatapoints_test.jsonl'\n",
    "    # creating groudtruth dictionary\n",
    "    default_ds.download('.', prefix=image_class_list_file_path, overwrite=True)\n",
    "    with open(image_class_list_file_path) as fp:\n",
    "        number_of_lines_in_validation_dataset = len(fp.readlines())\n",
    "        print(\"Number of lines in validation dataset: {}\".format(number_of_lines_in_validation_dataset))\n",
    "    ground_truth_with_filename = {}\n",
    "    with open(image_class_list_file_path) as ground_truth:\n",
    "        ground_truth_str = ground_truth.readline()\n",
    "        while ground_truth_str:\n",
    "            ground_truth_json_line = json.loads(ground_truth_str)\n",
    "            ground_truth_with_filename[ground_truth_json_line[\"image_url\"].replace(\"AmlDatastore://\", \"\")] = ground_truth_json_line[\"label\"]\n",
    "            ground_truth_str = ground_truth.readline()\n",
    "    return ground_truth_with_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in classification_tests:\n",
    "    prediction_dict = get_predictions(test['ExperimentName'], test['SoreRunId'])\n",
    "    ground_truth_dict = get_ground_truth(test['ExperimentName'])\n",
    "    assert len(ground_truth_dict) == len(prediction_dict)\n",
    "    for filename in prediction_dict.keys():\n",
    "        if filename in ground_truth_dict.keys():\n",
    "            test['y_pred'].append(prediction_dict[filename])\n",
    "            test['y_true'].append(ground_truth_dict[filename])\n",
    "        else:\n",
    "            print('Missing {} in ground trouth'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in classification_tests:\n",
    "    print('{} {}'.format(len(test['y_pred']), len(test['y_true'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "col = len(classification_tests)\n",
    "fig, ax = plt.subplots(1, col)\n",
    "fig.set_figwidth(8 * col)\n",
    "# fig.set_figheight(FIG_HEIGHT * total_rows)\n",
    "ax_index = 0\n",
    "for test in classification_tests:\n",
    "    cm = confusion_matrix(test['y_true'], test['y_pred'], labels= ['Lung Opacity', 'No Lung Opacity / Not Normal', 'Normal'])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Lung Opacity', 'No Lung Opacity / Not Normal', 'Normal'])\n",
    "    disp.plot(values_format='d', ax=ax[ax_index])\n",
    "    ax_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in classification_tests:\n",
    "    print(test['ExperimentName'])\n",
    "    print(classification_report(test['y_true'], test['y_pred'], target_names =  ['Lung Opacity', 'No Lung Opacity / Not Normal', 'Normal']))\n",
    "    print(\"=======================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}